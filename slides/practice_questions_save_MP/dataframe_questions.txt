1. Create the following DataFrame with 2 columns
and 3 rows:

df.show()

+---------+-----------+
| name    | tricks    |
+---------+-----------+
| alex    | [1, 2]    |
| jane    | []        |
| ted     | [1, 2, 3] |  
+---------+-----------+

======
2. Given the following DataFrame:

+---------+-----------+
| name    | tricks    |
+---------+-----------+
| alex    | [1, 2]    |
| jane    | []        |
| ted     | [1, 2, 3] |  
+---------+-----------+

write a set of general transformations to produce the
following DataFrame, where the last column is the number
of tricks:

+---------+-----------+--------------+
| name    | tricks    | num_tricks   |
+---------+-----------+--------------+
| alex    | [1, 2]    |     2        |
| jane    | []        |     0        |
| ted     | [1, 2, 3] |     3        |
+---------+-----------+--------------+

======
3. Given the following DataFrame:

+---------+-----------+--------------+
| name    | tricks    | num_tricks   |
+---------+-----------+--------------+
| alex    | [1, 2]    |     2        |
| jane    | []        |     0        |
| ted     | [1, 2, 3] |     3        |
+---------+-----------+--------------+

write a set of general transformations to keep
the rows if the number of tricks is more than 1.

======
4. Given the following DataFrame:

+---------+-----------+--------------+
| name    | tricks    | num_tricks   |
+---------+-----------+--------------+
| alex    | [1, 2]    |     2        |
| jane    | []        |     0        |
| ted     | [1, 2, 3] |     3        |
+---------+-----------+--------------+

write a set of general transformations to 
find top-5 tricks.

======
5. Given billions of records, where each record
has the following format:

<record_number><,><gene_id><,><gene_value>

Assume that your input files reside in /tmp/data/ directory.
 
5.1 Create a DataFrame to represent our input as
DataFrame(record_number, gene_id, gene_value)

5.2 Write a set of transformations to find average
of gene_values per gene_id

5.3 Write a set of transformations to find (min, max)
of gene_values per gene_id

5.4 Write a set of transformations to keep only
positive gene_values.

======
6. Given billions of records, where each record
has the following format:

<record_number><,><gene_id><,><gene_value>

Assume that your input files reside in /tmp/data/ directory.
 
6.1 Create a DataFrame to represent our input as
DataFrame(gene_id, gene_value)

6.2 Write a set of transformations to find median
of gene_values per gene_id

6.3 Write a set of transformations to find (N, P)
of gene_values per gene_id, where P denotes positive
gene_values and N denotes negative gene values

===
7. Given the following DataFrame:

+---------+-----------+--------------+
| name    | tricks    | num_tricks   |
+---------+-----------+--------------+
| alex    | [1, 2]    |     2        |
| jane    | []        |     0        |
| ted     | [1, 2, 3] |     3        |
+---------+-----------+--------------+

7.1 Keep records if name is not null.

7.2 find unique names, with no tricks

7.3 find unique names, with more than 1 tricks

======
8. Let genes.txt be a huge text file, where every 
record has the following format (reference can be 
in {1, 2, 3} where 1 denotes cancer, 2 denotes 
healthy, and 3 is undefined):

<geneID><,><reference><,><geneValue>

For example, a sample input might be:

g1,1,2.3
g1,2,1.5
g1,3,2.5
g1,1,4.1
g2,1,1.3
g2,2,1.8
g2,3,3.5
g2,1,4.3
g2,1,2.9
...
...
...
a. Create a DataFrame from input

b. Write a PySpark command/tranformation to
convert genes.txt file into an RDD<String> 
and then output the number of elements in 
that RDD

c. Let df represents the genes.txt file in 
a Spark DataFrame.  Use df and write a set of
tranformations to generate the following output 
per geneID

<geneID> <C> <S>

where C is the number of cancer genes (for geneID) 
and S is the sum of values for the cancer gene.

d. Use df and write a PySpark filter to remove all 
undefined genes.

======

9. Assume that all of the input is in a file 
called  "movies.txt" and each input record has the 
following format:

<userID><,><movieID><,><rating-in-range-of-1-to-5>

Sample input:

user1,movie1,3
user1,movie1,1
user1,movie2,5
user2,movie1,4
...


Note that a user may rate the same movie any 
number of times.

a. Represent data as a DataFrame (df)

b. Find the number of raters per movie.  
Write a complete PySpark program 
(as a set of trasformations and actions) to 
accomplish this task. Your output will be

<movieID> <number-of-raters>

c. The goal is to find the number of unique 
movies rated by each user. Write a complete 
PySpark program (as a set of trasformations 
and actions) to accomplish this task. Your 
output will be

<userID> <number-of-unique-movies>

===
10. Assume the following input
<Employee-ID><,><type>

where type can be:
"fulltime"
"parttime"
"contractor"

a. Create a DataFrame from input

b. Drop all duplicate records

c. find the total number "fulltime" and
"parttime" employees

======
11. Since a DataFrame is a table of rows
and named columns (similar to a relational
table), therefore we should not care about
partitioning a DataFrame into partitions.
Justify your answer.

=======

12. Assume that all of the input is in 
a file called  "movies.txt" and each 
input record has the following format:

<userID><,><movieID><,><rating-in-range-of-1-to-5>

Sample input:

user1,movie1,3
user1,movie1,1
user1,movie2,5
user2,movie1,4
...
...
...


Note that a user may rate the same movie any 
number of times. 

a. represent movies.txt file as a  DataFrame (df)

b. The goal is to find the number of raters 
per movie.  Write a complete PySpark program 
(as a set of trasformations and actions) to 
accomplish this task. Your output will be

<movieID> <number-of-raters>

c. The goal is to find the number of unique 
movies rated by each user. Write a complete 
PySpark program (as a set of trasformations 
and actions) to accomplish this task. 
Your output will be

<userID> <number-of-unique-movies>

======

13. Use PySpark to answer this question
Assume that all of the input is in a file called  
"movies.txt" (with millions of records) and each 
input record has the following format:

<MOVIE-ID><,><rating-in-range-of-1-to-5>

Sample input:

movie1,3
movie1,1
movie1,5
movie2,5
movie2,4
movie2,3
...


Note that a user may rate the same movie any number of 
times.  You HAVE to use the following Python functions 
in your transformations (NOTE, you MUST NOT use the 
split() function at all).

a. represent movies.txt file as a  DataFrame (df)

b. The goal is to find the number of raters 
per movie.  Write a complete PySpark program (as a set of 
PySpark trasformations and actions) to accomplish this task. 
Your output will be like:

<MOVIE-ID> <number-of-raters>

c. The goal is to find the average rating 
per movie. Write a complete PySpark program (as a set of 
trasformations and actions) to accomplish this task. Your 
output will be as:

<MOVIE-ID> <average-rating-per-MOVIE-ID>

===

14. Given a graph as edges:

<src_node><,><dst_node>

Using DataFrames, find inDegrees 
and outDegrees of all nodes.

The in-degree of each vertex in the graph, 
returned as a DataFame with two columns:
“id”: the ID of the vertex
“inDegree” (int) storing the in-degree of the vertex
Note that vertices with 0 in-edges are not returned 
in the result.

The out-degree of each vertex in the graph, returned 
as a DataFrame with two columns:
“id”: the ID of the vertex
“outDegree” (integer) storing the out-degree of the vertex 
Note that vertices with 0 out-edges are not returned 
in the result.

===

15. Consider the following SQL query:

SELECT COUNT(CustomerID), Country
    FROM Customers
        GROUP BY Country;

If the Customers table dumped as <CustomerID><,><Country>

How would you translate this SQL query by

a. PySpark RDDs
b. PySpark DataFrames

===
16. Consider the following SQL query:

SELECT COUNT(CustomerID) as COUNTED, Country
FROM Customers
GROUP BY Country
ORDER BY COUNTED DESC
LIMIT 5;

If the Customers table dumped as <CustomerID><,><Country>

How would you translate this SQL query by

a. PySpark RDDs
b. PySpark DataFrames

===
17. Consider the following SQL query:

SELECT NAME, SUM(SALARY) FROM Employee 
GROUP BY NAME
HAVING SUM(SALARY) > 3000; 

If the Employee table dumped as <NAME><,><SALARY>

How would you translate this SQL query by

a. PySpark RDDs
b. PySpark DataFrames

===

18. Create a DataFrame with the following columns:
(dept, name, salary), your data frame should have 
at least 4 rows.

77.1 Find average of salary per dept.
77.2 Find maximum of salary per dept.
77.3 Find minimum of salary per dept.
77.4 find (minimum, maximum) salary per dept.

===

19. Let e be a DataFrame representing edges
for a graph :
 e = (src_node, dst_node, weight)

Write a series of transformations to make this
graph undirected.

===

20. Let an RDD be of triplets (id, name, salary).
Then write a series of transformations to convert 
RDD into a DataFrame with 3 columns:  (id, name, salary)

===

21. Given a csv file with the following fileds:
continent, country, city, temperature

1 Create a DataFrame with 4 columns

2 Find average temperature per continent

3 Find average temperature per (continent, country)

4 Partition data in such a way that 80%
     of queries will analyze data by continent

5 Partition data in such a way that 70%
     of queries will analyze data by (continent, country).
     What will be table schema?

6 Partition data in such a way that 70%
     of queries will analyze data by 
     (continent) OR (continent, country, city)
     What will be table schema?
     
===

22.  Consider the following SQL query:

SELECT COUNT(CustomerID) as COUNTED, Continent, Country
FROM Customers
GROUP BY Continent, Country
ORDER BY COUNTED DESC
LIMIT 5;

If the Customers table dumped as <CustomerID><,><continent><,><Country>

How would you translate this SQL query by

a. PySpark RDDs
b. PySpark DataFrames

===

23. Consider the following DataFrame:

features = [('alex', 1), ('bob', 3), ('ali', 6), ('dave', 10)]
columns = ("name", "age")
samples = spark.createDataFrame(features, columns)
>>> samples.show()
+----+---+
|name|age|
+----+---+
|alex|  1|
| bob|  3|
| ali|  6|
|dave| 10|
+----+---+

How would you standardize the age column:
where 

age_scaled = (age - mean_age) / standard_deviation_age

The output should be:

+----+---+-------------------+
|name|age|age_scaled         |
+----+---+-------------------+
|alex|1  |-1.0215078369104984|
|bob |3  |-0.5107539184552492|
|ali |6  |0.2553769592276246 |
|dave|10 |1.276884796138123  |
+----+---+-------------------+

===

