# cat  data_kv.txt
1,alex
2,alex
3,david
3,david
4,jane
6,jane

# cd /home/spark-1.2.0/
# ./bin/pyspark
Python 2.6.9 (unknown, Sep  9 2014, 15:05:12)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin

>>> rdd1 = sc.textFile("data_kv.txt");
>>> list1 = rdd1.collect();

>>> list1
[
 u'1,alex', 
 u'2,alex', 
 u'3,david', 
 u'3,david', 
 u'4,jane', 
 u'6,jane'
]

>>> rdd2 = rdd1.filter(lambda s: "alex" in s);
>>> list2 = rdd2.collect();

>>> list2
[
 u'1,alex', 
 u'2,alex'
]

>>> rdd2 = rdd1.flatMap(lambda line: line.split(","))

>>> list2
[
 u'1', 
 u'alex', 
 u'2', 
 u'alex', 
 u'3', 
 u'david', 
 u'3', 
 u'david', 
 u'4', 
 u'jane', 
 u'6', 
 u'jane'
]
>>> list1 = rdd1.collect();

>>> list1
[
 u'1,alex', 
 u'2,alex', 
 u'3,david', 
 u'3,david', 
 u'4,jane', 
 u'6,jane'
]

>>>
 rdd2 = rdd1.map(lambda x: (x.split(",")[0], x.split(",")[1]))
>>> list2 = rdd2.collect();

>>> list2
[
 (u'1', u'alex'), 
 (u'2', u'alex'), 
 (u'3', u'david'), 
 (u'3', u'david'), 
 (u'4', u'jane'), 
 (u'6', u'jane')
]
>>> rdd3 = rdd2.groupByKey();
>>> list3 = rdd3.collect();

>>> list3
[
 (u'1', <pyspark.resultiterable.ResultIterable object at 0x10b44e210>), 
 (u'3', <pyspark.resultiterable.ResultIterable object at 0x10b44e250>), 
 (u'2', <pyspark.resultiterable.ResultIterable object at 0x10b44e290>), 
 (u'4', <pyspark.resultiterable.ResultIterable object at 0x10b44e2d0>), 
 (u'6', <pyspark.resultiterable.ResultIterable object at 0x10b44e310>)
]

>>> rdd3.saveAsTextFile("/Users/mparsian/rdd3");

# ls -l rdd3/
total 16
-rw-r--r--  1 ...    0 Feb 18 12:21 _SUCCESS
-rw-r--r--  1 ...  140 Feb 18 12:21 part-00000
-rw-r--r--  1 ...  210 Feb 18 12:21 part-00001

# cat rdd3/part*
(u'1', <pyspark.resultiterable.ResultIterable object at 0x104ba1f10>)
(u'3', <pyspark.resultiterable.ResultIterable object at 0x104ba1f10>)
(u'2', <pyspark.resultiterable.ResultIterable object at 0x104ba1f10>)
(u'4', <pyspark.resultiterable.ResultIterable object at 0x104ba1f10>)
(u'6', <pyspark.resultiterable.ResultIterable object at 0x104ba1f10>)


val rdd = sc.sequenceFile[String, Double](path)
rdd = sc.parallelize([('key1', 1.0), ('key2', 2.0), ('key3', 3.0)])
rdd.saveAsSequenceFile('/tmp/pysequencefile/')
...

sc.sequenceFile('/tmp/pysequencefile/').collect()
[
 (u'key1', 1.0), 
 (u'key2', 2.0), 
 (u'key3', 3.0)
]


# Examples:	Word	Count	
from pyspark.context import SparkContext
sc = SparkContext(...)
lines = sc.textFile(sys.argv[2], 1)
counts = lines
words = .map(lambda x: (x, 1)) \
        .reduceByKey(lambda x, y: x + y)

for (word, count) in counts.collect(): 
    print "%s : %i" % (word, count) 

lines.flatMap(lambda x: x.split(' ')) \
 .map(lambda x: (x, 1)) 
 
 
# Example: Word Count
from	pyspark.context	import	SparkContext
sc	=	SparkContext(...)
lines	=	sc.textFile(sys.argv[2],	1)
counts	=	lines.flatMap(lambda	x:	x.split('	'))	\										
	 	 	 	 		.map(lambda	x:	(x,	1))	\																												
	 	 	 	 		.reduceByKey(lambda	x,	y:	x	+	y)

for	(word,	count)	in	counts.collect(): 
    print	"%s	:	%i"	%	(word,	count)	
	 

When possible, RDD
transformations are pipelined:
lines.flatMap(lambda x: x.split(' ')) \
 .map(lambda x: (x, 1)) 


file = spark.textFile("hdfs://...")
errors = file.filter(lambda line: "ERROR" in line)
# Count all the errors
errors.count()
# Count errors mentioning MySQL
errors.filter(lambda line: "MySQL" in line).count()
# Fetch the MySQL errors as an array of strings
errors.filter(lambda line: "MySQL" in line).collect()

errors.cache()

Word Count
In this example, we use a few more transformations 
to build a dataset of (String, Int) pairs called 
counts and then save it to a file.

file = spark.textFile("hdfs://...")
counts = file.flatMap(lambda line: line.split(" ")) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
counts.saveAsTextFile("hdfs://...")
====================================================

# /home/zmp/zs/spark-1.2.0/bin/pyspark
Python 2.6.9 (unknown, Sep  9 2014, 15:05:12)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type "help", "copyright", "credits" or "license" for more information.

>>> sc
<pyspark.context.SparkContext object at 0x10ff25210>
>>> rdd1 = sc.textFile("data.txt");
>>> list1 = rdd1.collect();
>>> list1
[
 u'crazy crazy fox jumped', 
 u'crazy fox  jumped', 
 u'fox is fast', 
 u'fox is smart'
]
>>> rdd1.count();
4
>>> rdd2 = rdd1.map(lambda x: x.split(" "))
>>> list2 = rdd2.collect();
>>> list2
[
 [u'crazy', u'crazy', u'fox', u'jumped'], 
 [u'crazy', u'fox', u'', u'jumped'], 
 [u'fox', u'is', u'fast'], 
 [u'fox', u'is', u'smart']
]
>>> ones = rdd2.map(lambda x: (x, 1))
>>> listones = ones.collect();
>>> listones
[
 ([u'crazy', u'crazy', u'fox', u'jumped'], 1), 
 ([u'crazy', u'fox', u'', u'jumped'], 1), 
 ([u'fox', u'is', u'fast'], 1), 
 ([u'fox', u'is', u'smart'], 1)
]

>>> lines = sc.textFile("data.txt", 1)
>>> debuglines = lines.collect();
>>> debuglines
[
 u'crazy crazy fox jumped', 
 u'crazy fox  jumped', 
 u'fox is fast', 
 u'fox is smart'
]
>>> words = lines.flatMap(lambda x: x.split(' '))
>>> debugwords = words.collect();
>>> debugwords
[
 u'crazy', 
 u'crazy', 
 u'fox', 
 u'jumped', 
 u'crazy', 
 u'fox', 
 u'', 
 u'jumped', 
 u'fox', 
 u'is', 
 u'fast', 
 u'fox', 
 u'is', 
 u'smart'
]
>>> ones = words.map(lambda x: (x, 1))
>>> debugones = ones.collect()
>>> debugones
[
 (u'crazy', 1), 
 (u'crazy', 1), 
 (u'fox', 1), 
 (u'jumped', 1), 
 (u'crazy', 1), 
 (u'fox', 1), 
 (u'', 1), 
 (u'jumped', 1), 
 (u'fox', 1), 
 (u'is', 1), 
 (u'fast', 1), 
 (u'fox', 1), 
 (u'is', 1), 
 (u'smart', 1)
]
>>> counts = ones.reduceByKey(lambda x, y: x + y)
>>> debugcounts = counts.collect()
>>> debugcounts
[
 (u'', 1), 
 (u'crazy', 3), 
 (u'jumped', 2), 
 (u'is', 2), 
 (u'fox', 4), 
 (u'fast', 1), 
 (u'smart', 1)
]
>>>
===============================

interactive session: valid and tested: Feb. 23, 2015

# cat data.txt
crazy crazy fox jumped
crazy fox jumped
fox is fast
fox is smart
dog is smart

# ./bin/pyspark
Python 2.6.9 (unknown, Sep  9 2014, 15:05:12)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type "help", "copyright", "credits" or "license" for more information.

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.2.0
      /_/

Using Python version 2.6.9 (unknown, Sep  9 2014 15:05:12)
SparkContext available as sc.
>>> sc
<pyspark.context.SparkContext object at 0x10ae02210>
>>> lines = sc.textFile("data.txt", 1)
>>> debuglines = lines.collect();
>>> debuglines
[
 u'crazy crazy fox jumped', 
 u'crazy fox jumped', 
 u'fox is fast', 
 u'fox is smart', 
 u'dog is smart'
]
>>> words = lines.flatMap(lambda x: x.split(' '))
>>> debugwords = words.collect();
>>> debugwords
[
 u'crazy', 
 u'crazy', 
 u'fox', 
 u'jumped', 
 u'crazy', 
 u'fox', 
 u'jumped', 
 u'fox', 
 u'is', 
 u'fast', 
 u'fox', 
 u'is', 
 u'smart', 
 u'dog', 
 u'is', 
 u'smart'
]
>>> ones = words.map(lambda x: (x, 1))
>>> debugones = ones.collect()
>>> debugones
[
 (u'crazy', 1), 
 (u'crazy', 1), 
 (u'fox', 1), 
 (u'jumped', 1), 
 (u'crazy', 1), 
 (u'fox', 1), 
 (u'jumped', 1), 
 (u'fox', 1), 
 (u'is', 1), 
 (u'fast', 1), 
 (u'fox', 1), 
 (u'is', 1), 
 (u'smart', 1), 
 (u'dog', 1), 
 (u'is', 1), 
 (u'smart', 1)
]
>>> counts = ones.reduceByKey(lambda x, y: x + y)
>>> debugcounts = counts.collect()
>>> debugcounts
[
 (u'crazy', 3), 
 (u'jumped', 2), 
 (u'is', 3), 
 (u'fox', 4), 
 (u'dog', 1), 
 (u'fast', 1), 
 (u'smart', 2)
]
>>>

>>> grouped = ones.groupByKey();
>>> debuggrouped = grouped.collect();

>>> counts.saveAsTextFile("output.txt")

# cat output.txt/part*
(u'crazy', 3)
(u'jumped', 2)
(u'is', 3)
(u'fox', 4)
(u'dog', 1)
(u'fast', 1)
(u'smart', 2)