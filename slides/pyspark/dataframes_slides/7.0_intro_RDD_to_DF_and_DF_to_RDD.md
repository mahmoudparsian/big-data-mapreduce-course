# (RDD to DataFrame) and (DataFrame to RDD)

* An RDD can be converted to a DataFrame: 
	* make every item as a Row object

* DataFrame can be converted to an RDD

~~~
~  % ./spark-3.5.3/bin/pyspark
Python 3.12.0 (v3.12.0:0fb18b02c8, Oct  2 2023, 09:45:56) 
[Clang 13.0.0 (clang-1300.0.29.30)] on darwin
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.3
      /_/

Spark context Web UI available at http://172.20.218.221:4040
Spark context available as 'sc' (master = local[*], app id = local-1730867710100).
SparkSession available as 'spark'.
>>>
>>> #-------------------------------------------------
>>> # Create a Row object: 
>>> # Row is like a dictionary of keys and values
>>> #-------------------------------------------------
>>>
>>> # Import required library
>>> from pyspark.sql import Row

>>> # Create a Row object as row1
>>> row1 = Row(name="Alice", age=30)
>>> row1
Row(name='Alice', age=30)

>>> #-------------------------------------------------
>>> # Create an RDD an then convert it to a DataFrame
>>> #-------------------------------------------------
>>> key_value = [('alex', 30, 30000), ('jane', 40, 40000), 
                 ('mat', 60, 60000), ('janet', 60, 90000)]
>>> key_value
[('alex', 30, 30000), ('jane', 40, 40000), 
 ('mat', 60, 60000), ('janet', 60, 90000)]
>>>
>>> #-------------------------------------------------
>>> # Create an RDD of triplets
>>> #-------------------------------------------------
>>> rdd = sc.parallelize(key_value)
>>> rdd.collect()
[
 ('alex', 30, 30000), 
 ('jane', 40, 40000), 
 ('mat', 60, 60000), 
 ('janet', 60, 90000)
]
>>> rdd.count()
4

>>> #-------------------------------------------------
>>> # Map each element of an RDD to be a Row object
>>> #-------------------------------------------------
>>> rdd2 = rdd.map(lambda x: Row(name=x[0], age=x[1], salary=x[2]))
>>> rdd2.collect()
[
 Row(name='alex', age=30, salary=30000), 
 Row(name='jane', age=40, salary=40000), 
 Row(name='mat', age=60, salary=60000), 
 Row(name='janet', age=60, salary=90000)
]
>>>
>>> #-------------------------------------------------
>>> # Convert an RDD of Rows to a DataFrame
>>> #-------------------------------------------------
>>> df = rdd2.toDF()
>>> df.show()
+-----+---+------+
| name|age|salary|
+-----+---+------+
| alex| 30| 30000|
| jane| 40| 40000|
|  mat| 60| 60000|
|janet| 60| 90000|
+-----+---+------+

>>> df.printSchema()
root
 |-- name: string (nullable = true)
 |-- age: long (nullable = true)
 |-- salary: long (nullable = true)

>>> #-------------------------------------------------
>>> # Register your DataFrame as a table named "emps"
>>> #-------------------------------------------------
>>> df.createOrReplaceTempView("emps")
>>>

>>> #-------------------------------------------------
>>> # Execute a SQL query using table "emps"
>>> #-------------------------------------------------
>>> spark.sql("select * from emps").show()
+-----+---+------+
| name|age|salary|
+-----+---+------+
| alex| 30| 30000|
| jane| 40| 40000|
|  mat| 60| 60000|
|janet| 60| 90000|
+-----+---+------+

>>> spark.sql("select * from emps where salary > 45000").show()
+-----+---+------+
| name|age|salary|
+-----+---+------+
|  mat| 60| 60000|
|janet| 60| 90000|
+-----+---+------+

>>>
>>> df2 = spark.sql("select * from emps where salary > 45000")
>>> df2.show()
+-----+---+------+
| name|age|salary|
+-----+---+------+
|  mat| 60| 60000|
|janet| 60| 90000|
+-----+---+------+

>>>
>>> #-------------------------------------------------
>>> # Now, convert a DataFrame to an RDD
>>> #-------------------------------------------------
>>> rdd_converted = df2.rdd
>>> rdd_converted.collect()
[
 Row(name='mat', age=60, salary=60000), 
 Row(name='janet', age=60, salary=90000)
]
>>>
>>> #-------------------------------------------------
>>> # Remove Row from each element
>>> #-------------------------------------------------
>>> rdd3 = rdd_converted.map(lambda r: (r.name, r.age, r.salary))
>>> rdd3.collect()
[
 ('mat', 60, 60000), 
 ('janet', 60, 90000)
]
>>>
~~~